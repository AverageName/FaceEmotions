{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35887"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/max/fer2013/fer2013.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       emotion                                             pixels     Usage\n",
      "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
      "1            0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
      "2            2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
      "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
      "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
      "5            2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n",
      "6            4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n",
      "7            3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n",
      "8            3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training\n",
      "9            2  255 254 255 254 254 179 122 107 95 124 149 150...  Training\n",
      "10           0  30 24 21 23 25 25 49 67 84 103 120 125 130 139...  Training\n",
      "11           6  39 75 78 58 58 45 49 48 103 156 81 45 41 38 49...  Training\n",
      "12           6  219 213 206 202 209 217 216 215 219 218 223 23...  Training\n",
      "13           6  148 144 130 129 119 122 129 131 139 153 140 12...  Training\n",
      "14           3  4 2 13 41 56 62 67 87 95 62 65 70 80 107 127 1...  Training\n",
      "15           5  107 107 109 109 109 109 110 101 123 140 144 14...  Training\n",
      "16           3  14 14 18 28 27 22 21 30 42 61 77 86 88 95 100 ...  Training\n",
      "17           2  255 255 255 255 255 255 255 255 255 255 255 25...  Training\n",
      "18           6  134 124 167 180 197 194 203 210 204 203 209 20...  Training\n",
      "19           4  219 192 179 148 208 254 192 98 121 103 145 185...  Training\n",
      "20           4  1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 7 12 23 45 38 ...  Training\n",
      "21           2  174 51 37 37 38 41 22 25 22 24 35 51 70 83 98 ...  Training\n",
      "22           0  123 125 124 142 209 226 234 236 231 232 235 22...  Training\n",
      "23           0  8 9 14 21 26 32 37 46 52 62 72 70 71 73 76 83 ...  Training\n",
      "24           3  252 250 246 229 182 140 98 72 53 44 67 95 95 8...  Training\n",
      "25           3  224 227 219 217 215 210 187 177 189 200 206 21...  Training\n",
      "26           5  162 200 187 180 197 198 196 192 176 152 136 11...  Training\n",
      "27           0  236 230 225 226 228 209 199 193 196 211 199 19...  Training\n",
      "28           3  210 210 210 210 211 207 147 103 68 60 47 70 12...  Training\n",
      "29           5  50 44 74 141 187 187 169 113 80 128 181 172 76...  Training\n",
      "...        ...                                                ...       ...\n",
      "28679        6  39 39 39 39 38 30 41 63 105 117 84 97 101 75 5...  Training\n",
      "28680        4  137 146 153 157 164 166 169 172 177 176 176 17...  Training\n",
      "28681        6  208 207 205 206 207 207 210 211 210 207 211 21...  Training\n",
      "28682        2  10 10 10 10 10 10 10 10 10 10 12 2 45 117 122 ...  Training\n",
      "28683        4  178 142 131 130 145 152 125 92 115 142 149 158...  Training\n",
      "28684        3  80 94 86 71 98 74 46 67 105 71 63 76 51 53 80 ...  Training\n",
      "28685        4  94 92 91 92 93 93 92 92 90 90 61 41 34 37 52 6...  Training\n",
      "28686        0  178 184 187 195 199 194 197 205 202 194 201 20...  Training\n",
      "28687        3  114 100 121 166 185 175 160 174 195 205 216 22...  Training\n",
      "28688        3  30 47 52 25 29 48 46 41 70 63 66 49 37 41 35 3...  Training\n",
      "28689        3  181 178 179 171 78 51 56 46 48 50 54 54 68 96 ...  Training\n",
      "28690        2  186 182 173 164 164 177 91 45 66 72 79 79 85 1...  Training\n",
      "28691        4  255 255 255 255 255 255 255 255 255 255 255 25...  Training\n",
      "28692        3  99 103 106 109 112 113 115 118 120 121 115 83 ...  Training\n",
      "28693        6  216 219 216 209 181 104 128 129 134 136 135 14...  Training\n",
      "28694        3  159 195 167 158 152 150 149 154 154 151 149 14...  Training\n",
      "28695        2  84 96 110 132 165 183 175 154 116 95 75 67 63 ...  Training\n",
      "28696        4  0 0 1 1 7 35 76 87 86 90 87 83 89 92 92 93 98 ...  Training\n",
      "28697        3  181 172 161 144 116 109 70 109 187 131 67 30 2...  Training\n",
      "28698        3  35 45 69 79 75 48 45 35 56 93 71 51 48 47 46 4...  Training\n",
      "28699        6  128 134 164 94 70 114 159 138 75 47 89 127 134...  Training\n",
      "28700        4  11 10 12 13 9 11 10 11 11 10 10 13 11 10 11 10...  Training\n",
      "28701        2  34 42 47 34 53 41 33 39 42 38 40 44 41 42 42 4...  Training\n",
      "28702        0  196 194 188 177 156 124 81 60 65 64 84 119 114...  Training\n",
      "28703        5  255 255 255 255 255 255 255 203 145 147 143 14...  Training\n",
      "28704        2  84 85 85 85 85 85 85 85 86 86 86 87 86 86 91 9...  Training\n",
      "28705        0  114 112 113 113 111 111 112 113 115 113 114 11...  Training\n",
      "28706        4  74 81 87 89 95 100 98 93 105 120 127 133 146 1...  Training\n",
      "28707        0  222 227 203 90 86 90 84 77 94 87 99 119 134 14...  Training\n",
      "28708        4  195 199 205 206 205 203 206 209 208 210 212 21...  Training\n",
      "\n",
      "[28709 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df[df.iloc[:,2]=='Training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FaceEmotionDataset(Dataset):\n",
    "    def __init__(self,csv_file,root_dir,transform=None,train=True):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if train:\n",
    "            self.df = self.df[self.df.iloc[:,2]=='Training']\n",
    "        else:\n",
    "            self.df = self.df[self.df.iloc[:,2]!='Training']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        emotion = torch.LongTensor([int(self.df.iloc[idx,0])])\n",
    "        image = pd.DataFrame(float(x) for x in self.df.iloc[idx,1].split(' '))\n",
    "        image = image.as_matrix().reshape(-1,48)\n",
    "        image = np.expand_dims(image,axis = 2)\n",
    "        image = np.transpose(image,(2,0,1))\n",
    "        image = image.astype(np.float32)\n",
    "        return image,emotion\n",
    "        \n",
    "classes = ('0','1','2','3','4','5','6')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_1 = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "\n",
    "train_dataset = FaceEmotionDataset(csv_file='/home/max/fer2013/fer2013.csv',\n",
    "                                    root_dir='home/max/fer2013/',transform = transform_1,train=True)\n",
    "test_dataset = FaceEmotionDataset(csv_file='/home/max/fer2013/fer2013.csv',\n",
    "                                    root_dir='home/max/fer2013/',transform = transform_1,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,15,3)\n",
    "        #self.conv3 = nn.Conv2d(15,)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1500,64)\n",
    "        #self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(64,7)\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,1500)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = (self.fc3(x))\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net().cuda()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.001,momentum =0.9)\n",
    "from torch.autograd import Variable\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle = True,num_workers=4) \n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 ,   100] loss: 0.015\n",
      "[1 ,   200] loss: 0.015\n",
      "[1 ,   300] loss: 0.015\n",
      "[1 ,   400] loss: 0.016\n",
      "[2 ,   100] loss: 0.016\n",
      "[2 ,   200] loss: 0.015\n",
      "[2 ,   300] loss: 0.014\n",
      "[2 ,   400] loss: 0.016\n",
      "[3 ,   100] loss: 0.015\n",
      "[3 ,   200] loss: 0.015\n",
      "[3 ,   300] loss: 0.014\n",
      "[3 ,   400] loss: 0.016\n",
      "[4 ,   100] loss: 0.016\n",
      "[4 ,   200] loss: 0.013\n",
      "[4 ,   300] loss: 0.014\n",
      "[4 ,   400] loss: 0.016\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(4):\n",
    "    sum_loss = 0.0\n",
    "    for idx,data in enumerate(train_loader,0):\n",
    "            inputs,labels = data\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs,labels.squeeze_())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss +=loss.data[0]\n",
    "            if idx%100 == 99 :\n",
    "                print('[%d , %5d] loss: %.3f' %\n",
    "                  (epoch + 1,idx + 1,sum_loss /100 ))\n",
    "            sum_loss = 0.0\n",
    "print('Finished training')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test 39.96935079409306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "correct = 0 \n",
    "total = 0\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    inputs,labels = data\n",
    "    labels = labels.cuda()\n",
    "    inputs = Variable(inputs).cuda()\n",
    "    outputs = net(inputs)\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predictions == labels.squeeze_()).sum()\n",
    "print('Accuracy for test',100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
