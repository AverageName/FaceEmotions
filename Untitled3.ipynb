{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils,models\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35887"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/max/fer2013/fer2013.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When we disable transforms in __getitem__ everything works ok with 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=F size=224x224 at 0x7F417E3BD320>\n",
      "\n",
      "( 0 ,.,.) = \n",
      "  0.4679  0.4679  0.4508  ...  -0.0629 -0.0629 -0.0629\n",
      "  0.4679  0.4679  0.4508  ...  -0.0629 -0.0629 -0.0629\n",
      "  0.4679  0.4679  0.4508  ...  -0.0458 -0.0629 -0.0629\n",
      "           ...             ⋱             ...          \n",
      "  1.0673  1.0673  1.0673  ...   1.0331  1.0331  1.0331\n",
      "  1.0673  1.0673  1.0502  ...   1.0159  1.0331  1.0331\n",
      "  1.0673  1.0673  1.0502  ...   1.0159  1.0331  1.0331\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.6078  0.6078  0.5903  ...   0.0651  0.0651  0.0651\n",
      "  0.6078  0.6078  0.5903  ...   0.0651  0.0651  0.0651\n",
      "  0.6078  0.6078  0.5903  ...   0.0826  0.0651  0.0651\n",
      "           ...             ⋱             ...          \n",
      "  1.2206  1.2206  1.2206  ...   1.1856  1.1856  1.1856\n",
      "  1.2206  1.2206  1.2031  ...   1.1681  1.1856  1.1856\n",
      "  1.2206  1.2206  1.2031  ...   1.1681  1.1856  1.1856\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  0.8274  0.8274  0.8099  ...   0.2871  0.2871  0.2871\n",
      "  0.8274  0.8274  0.8099  ...   0.2871  0.2871  0.2871\n",
      "  0.8274  0.8274  0.8099  ...   0.3045  0.2871  0.2871\n",
      "           ...             ⋱             ...          \n",
      "  1.4374  1.4374  1.4374  ...   1.4025  1.4025  1.4025\n",
      "  1.4374  1.4374  1.4200  ...   1.3851  1.4025  1.4025\n",
      "  1.4374  1.4374  1.4200  ...   1.3851  1.4025  1.4025\n",
      "[torch.FloatTensor of size 3x224x224]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Everything here on a single input does work\n",
    "transfrom_kek = transforms.Compose([transforms.ToPILImage(),transforms.Resize(224)])\n",
    "img = transfrom_kek(train_res_dataset.__getitem__(1)[0])\n",
    "print(img)\n",
    "transform_toTens = transforms.ToTensor()\n",
    "img = img.convert(\"RGB\")\n",
    "img = transform_toTens(img)\n",
    "transform_Norm = transforms.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225))\n",
    "img = transform_Norm(img)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FaceEmotionDataset(Dataset):\n",
    "    def __init__(self,csv_file,root_dir,transform=None,train=True):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if train:\n",
    "            self.df = self.df[self.df.iloc[:,2]=='Training']\n",
    "        else:\n",
    "            self.df = self.df[self.df.iloc[:,2]!='Training']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        emotion = torch.LongTensor([int(self.df.iloc[idx,0])])\n",
    "        image = pd.DataFrame(float(x) for x in self.df.iloc[idx,1].split(' '))\n",
    "        image = image.as_matrix().reshape(-1,48)\n",
    "        image = np.expand_dims(image,axis = 2)\n",
    "        \n",
    "        #Disabled transforms\n",
    "        \n",
    "        #image = np.transpose(image,(2,0,1))\n",
    "        image = image.astype(np.float32)\n",
    "        #image = transforms.ToPILImage(image)\n",
    "        #image = image.convert(\"RGB\")\n",
    "        #if transform:\n",
    "            #image = self.transform(image)\n",
    "        return image,emotion\n",
    "        \n",
    "classes = ('0','1','2','3','4','5','6')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When enable transforms and trying to add first two transforms in __getitem__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FaceEmotionDataset(Dataset):\n",
    "    def __init__(self,csv_file,root_dir,transform=None,train=True):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if train:\n",
    "            self.df = self.df[self.df.iloc[:,2]=='Training']\n",
    "        else:\n",
    "            self.df = self.df[self.df.iloc[:,2]!='Training']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        emotion = torch.LongTensor([int(self.df.iloc[idx,0])])\n",
    "        image = pd.DataFrame(float(x) for x in self.df.iloc[idx,1].split(' '))\n",
    "        image = image.as_matrix().reshape(-1,48)\n",
    "        image = np.expand_dims(image,axis = 2)\n",
    "        #image = np.transpose(image,(2,0,1))\n",
    "        image = image.astype(np.float32)\n",
    "        image = transforms.ToPILImage(image)\n",
    "        image = image.convert(\"RGB\")\n",
    "        if transform:\n",
    "            image = self.transform(image)\n",
    "        return image,emotion\n",
    "        \n",
    "classes = ('0','1','2','3','4','5','6')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "input_ft = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(input_ft,7)\n",
    "optimizer_2 = optim.SGD(model_ft.parameters(),lr = 0.001,momentum=0.9,weight_decay=1e-5)\n",
    "exp_lr_sheduler = lr_scheduler.StepLR(optimizer_2,step_size=7,gamma=0.1)\n",
    "transform_resnet = transforms.Compose([transforms.Resize(224),transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225))])\n",
    "train_res_dataset = FaceEmotionDataset(csv_file='/home/max/fer2013/fer2013.csv',\n",
    "                                    root_dir='home/max/fer2013/',transform = transform_resnet,train=True)\n",
    "test_res_dataset = FaceEmotionDataset(csv_file='/home/max/fer2013/fer2013.csv',\n",
    "                                    root_dir='home/max/fer2013/',transform = transform_resnet,train=False)\n",
    "train_res_dataloader = DataLoader(train_res_dataset,batch_size=64,shuffle = True,num_workers=4)\n",
    "test_res_dataloader = DataLoader(test_res_dataset,batch_size=64,shuffle = False,num_workers=4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train (model,criterion,optimizer,scheduler,num_epoches):\n",
    "    model.train()\n",
    "    for epoch in range(num_epoches):\n",
    "        scheduler.step()\n",
    "        sum_loss = 0.0\n",
    "        for idx,data in enumerate(train_res_dataloader,0):\n",
    "            inputs,labels = data\n",
    "            print(inputs)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda().squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "\n",
    "        \n",
    "            loss = criterion(outputs,labels)\n",
    "            sum_loss +=loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if idx%100 == 99:\n",
    "                print('Epoch: ',epoch,'Num: ',idx)\n",
    "                print('Loss: ',sum_loss/100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/max/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/max/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-3-6d93aa9ef70d>\", line 22, in __getitem__\n    image = image.convert(\"RGB\")\nAttributeError: 'ToPILImage' object has no attribute 'convert'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-af6e6f9507c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp_lr_sheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2a0249449d27>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, scheduler, num_epoches)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Traceback (most recent call last):\n  File \"/home/max/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/max/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-3-6d93aa9ef70d>\", line 22, in __getitem__\n    image = image.convert(\"RGB\")\nAttributeError: 'ToPILImage' object has no attribute 'convert'\n"
     ]
    }
   ],
   "source": [
    "train (model_ft,criterion,optimizer_2,exp_lr_sheduler,10)\n",
    "#Strange error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_1 = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "train_dataset = FaceEmotionDataset(csv_file='/home/max/fer2013/fer2013.csv',\n",
    "                                    root_dir='home/max/fer2013/',transform = transform_1,train=True)\n",
    "test_dataset = FaceEmotionDataset(csv_file='/home/max/fer2013/fer2013.csv',\n",
    "                                    root_dir='home/max/fer2013/',transform = transform_1,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,15,3)\n",
    "        #self.conv3 = nn.Conv2d(15,)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1500,64)\n",
    "        #self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(64,7)\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,1500)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = (self.fc3(x))\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net().cuda()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.001,momentum =0.9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle = True,num_workers=4) \n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 ,   100] loss: 0.002\n",
      "[1 ,   200] loss: 0.002\n",
      "[1 ,   300] loss: 0.006\n",
      "[1 ,   400] loss: 0.003\n",
      "[2 ,   100] loss: 0.002\n",
      "[2 ,   200] loss: 0.003\n",
      "[2 ,   300] loss: 0.002\n",
      "[2 ,   400] loss: 0.003\n",
      "[3 ,   100] loss: 0.003\n",
      "[3 ,   200] loss: 0.006\n",
      "[3 ,   300] loss: 0.002\n",
      "[3 ,   400] loss: 0.003\n",
      "[4 ,   100] loss: 0.001\n",
      "[4 ,   200] loss: 0.001\n",
      "[4 ,   300] loss: 0.003\n",
      "[4 ,   400] loss: 0.002\n",
      "[5 ,   100] loss: 0.005\n",
      "[5 ,   200] loss: 0.005\n",
      "[5 ,   300] loss: 0.003\n",
      "[5 ,   400] loss: 0.003\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    sum_loss = 0.0\n",
    "    for idx,data in enumerate(train_loader,0):\n",
    "            inputs,labels = data\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs,labels.squeeze_())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss +=loss.data[0]\n",
    "            if idx%100 == 99 :\n",
    "                print('[%d , %5d] loss: %.3f' %\n",
    "                  (epoch + 1,idx + 1,sum_loss /100 ))\n",
    "            sum_loss = 0.0\n",
    "print('Finished training')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test 38.562273613820004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "correct = 0 \n",
    "total = 0\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    inputs,labels = data\n",
    "    labels = labels.cuda()\n",
    "    inputs = Variable(inputs).cuda()\n",
    "    outputs = net(inputs)\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predictions == labels.squeeze_()).sum()\n",
    "print('Accuracy for test',100 * correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
